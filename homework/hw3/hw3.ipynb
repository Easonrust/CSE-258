{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks(Cook/Make prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s split the training data (‘trainInteractions.csv.gz’) as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Reviews 1-400,000 for training \\\n",
    "(2) Reviews 400,000-500,000 for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Evaluate the performance (accuracy) of the baseline model on the validation set you have built "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import random\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"assignment1/trainInteractions.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    c = csv.reader(f)\n",
    "    header = next(c)\n",
    "    for l in c:\n",
    "        d = dict(zip(header,l))\n",
    "        yield d['user_id'],d['recipe_id'],d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(readCSV(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('88348277',\n",
       " '03969194',\n",
       " {'user_id': '88348277',\n",
       "  'recipe_id': '03969194',\n",
       "  'date': '2004-12-23',\n",
       "  'rating': '5'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsPerUser = defaultdict(set)\n",
    "usersPerItem=defaultdict(set)\n",
    "itemSet=set([d[1] for d in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    user,item = d[0], d[1]\n",
    "    itemsPerUser[user].add(item)\n",
    "    usersPerItem[item].add(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_validate_set(dataset):\n",
    "    validate_set=[]\n",
    "    random.seed(50)\n",
    "    for d in dataset:\n",
    "        positive_entry=[d[0],d[1],1]\n",
    "        negative_entry_item_set=itemSet.difference(itemsPerUser[d[0]])\n",
    "        random_item=random.choice(list(negative_entry_item_set))\n",
    "        negative_entry=[d[0],random_item,0]\n",
    "        validate_set.append(positive_entry)\n",
    "        validate_set.append(negative_entry)\n",
    "    return validate_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_set(dataset):\n",
    "    train_set=[]\n",
    "    for d in dataset:\n",
    "        positive_entry=[d[0],d[1]]\n",
    "        train_set.append(positive_entry)\n",
    "    return train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=build_train_set(dataset[:400000])\n",
    "validate_set=build_validate_set(dataset[400000:500000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['88348277', '03969194'],\n",
       " ['86699739', '27096427'],\n",
       " ['03425965', '44197323'],\n",
       " ['73973193', '24971400'],\n",
       " ['15215209', '60170202'],\n",
       " ['75799794', '39662395'],\n",
       " ['77745222', '88709727'],\n",
       " ['80598779', '09359141'],\n",
       " ['35769308', '83909791'],\n",
       " ['31763244', '20530585']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['90764166', '01768679', 1],\n",
       " ['90764166', '76921392', 0],\n",
       " ['68112239', '24923981', 1],\n",
       " ['68112239', '23778772', 0],\n",
       " ['32173358', '57597698', 1],\n",
       " ['32173358', '68548852', 0],\n",
       " ['30893740', '16266088', 1],\n",
       " ['30893740', '52444283', 0],\n",
       " ['69780905', '62953151', 1],\n",
       " ['69780905', '28155581', 0]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_set[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base line model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipeCount = defaultdict(int)\n",
    "totalCooked = 0\n",
    "\n",
    "# for user,recipe,_ in readCSV(\"assignment1/trainInteractions.csv.gz\"):\n",
    "#   recipeCount[recipe] += 1\n",
    "#   totalCooked += 1\n",
    "for d in train_set:\n",
    "    recipeCount[d[1]] += 1\n",
    "    totalCooked += 1\n",
    "\n",
    "mostPopular = [(recipeCount[x], x) for x in recipeCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalCooked/2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size=len(validate_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_size=0\n",
    "for i in range(total_size):\n",
    "    sample=validate_set[i]\n",
    "    item=sample[1]\n",
    "    predict=0\n",
    "    if item in return1:\n",
    "        predict=1\n",
    "    if predict==sample[2]:\n",
    "        correct_size+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.669945"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy=correct_size/total_size\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. See if you can find a better threshold and report its performance on your validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model_accuracy(threshhold):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "      count += ic\n",
    "      return1.add(i)\n",
    "      if count > totalCooked*threshhold: break\n",
    "    correct_size=0\n",
    "    for i in range(total_size):\n",
    "        sample=validate_set[i]\n",
    "        item=sample[1]\n",
    "        predict=0\n",
    "        if item in return1:\n",
    "            predict=1\n",
    "        if predict==sample[2]:\n",
    "            correct_size+=1\n",
    "    accuracy=correct_size/total_size\n",
    "    print([accuracy,threshhold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go through 1/10 to 9/10 to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.547455, 0.1]\n",
      "[0.59083, 0.2]\n",
      "[0.627175, 0.30000000000000004]\n",
      "[0.65384, 0.4]\n",
      "[0.669945, 0.5]\n",
      "[0.67576, 0.6000000000000001]\n",
      "[0.66676, 0.7000000000000001]\n",
      "[0.63734, 0.8]\n",
      "[0.5579, 0.9]\n",
      "[0.46477, 1.0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    baseline_model_accuracy((i+1)*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that when threshhold is 0.6*totalCooked, the accuracy improve to 0.67576, so 0.6*totalCooked is a better threshhold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. A stronger baseline than the one provided might make use of the Jaccard similarity (or another similarity\n",
    "metric). Given a pair (u,g) in the validation set, consider all training items g′ that user u has cooked. For each, compute the Jaccard similarity between g and g′, i.e., users (in the training set) who have made ′\n",
    "g and users who have made g . Predict as ‘made’ if the maximum of these Jaccard similarities exceeds a threshold (you may choose the threshold that works best). Report the performance on your validation set (1 mark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to redefined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsPerUser = defaultdict(set)\n",
    "usersPerItem=defaultdict(set)\n",
    "for d in train_set:\n",
    "    user,item = d[0], d[1]\n",
    "    itemsPerUser[user].add(item)\n",
    "    usersPerItem[item].add(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictUsingJaccard(item,user,t):\n",
    "    predict=0\n",
    "    maxSim=0\n",
    "    for d in itemsPerUser[user]:\n",
    "        sim=Jaccard(usersPerItem[d],usersPerItem[item])\n",
    "        maxSim=max(maxSim,sim)\n",
    "    if maxSim>t:\n",
    "        predict=1\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_model_accuracy(t):\n",
    "    total_size=len(validate_set)\n",
    "    correct_size=0\n",
    "    for i in range(total_size):\n",
    "        sample=validate_set[i]\n",
    "        item=sample[1]\n",
    "        user=sample[0]\n",
    "        predict=predictUsingJaccard(item,user,t)\n",
    "        if predict==sample[2]:\n",
    "            correct_size+=1\n",
    "    accuracy=correct_size/total_size\n",
    "    print([accuracy,t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go through t from 1/10 to 10/10 to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49967, 0.1]\n",
      "[0.493395, 0.2]\n",
      "[0.491505, 0.30000000000000004]\n",
      "[0.494635, 0.4]\n",
      "[0.50011, 0.5]\n",
      "[0.500085, 0.6000000000000001]\n",
      "[0.500005, 0.7000000000000001]\n",
      "[0.5, 0.8]\n",
      "[0.5, 0.9]\n",
      "[0.5, 1.0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    jaccard_model_accuracy((i+1)*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we go through t from 1/100 to 1/10 to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.593985, 0.01]\n",
      "[0.584725, 0.02]\n",
      "[0.561155, 0.03]\n",
      "[0.535635, 0.04]\n",
      "[0.51912, 0.05]\n",
      "[0.510805, 0.06]\n",
      "[0.50634, 0.07]\n",
      "[0.503075, 0.08]\n",
      "[0.50168, 0.09]\n",
      "[0.49967, 0.1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    jaccard_model_accuracy((i+1)*0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we go through t from 1/1000 to 1/100 to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.588305, 0.001]\n",
      "[0.58885, 0.002]\n",
      "[0.590135, 0.003]\n",
      "[0.591505, 0.004]\n",
      "[0.59221, 0.005]\n",
      "[0.592555, 0.006]\n",
      "[0.593085, 0.007]\n",
      "[0.59371, 0.008]\n",
      "[0.59386, 0.009000000000000001]\n",
      "[0.593985, 0.01]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    jaccard_model_accuracy((i+1)*0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we use 0.01 as our threshhold, and the accuracy on the validation set is 0.595145."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Improve the above predictor by incorporating both a Jaccard-based threshold and a popularity based threshold. Report the performance on your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardPopularityModel(train_set, test_set, jt=0.01, pt=0.6):\n",
    "    itemsPerUser = defaultdict(set)\n",
    "    usersPerItem=defaultdict(set)\n",
    "    for d in train_set:\n",
    "        user,item = d[0], d[1]\n",
    "        itemsPerUser[user].add(item)\n",
    "        usersPerItem[item].add(user)\n",
    "    \n",
    "    itemSet=set([d[1] for d in train_set])\n",
    "    userSet=set([d[0] for d in train_set])\n",
    "    \n",
    "    # calculate average number of recipes made in the train_set\n",
    "    averageNum=len(train_set)/len(userSet)\n",
    "\n",
    "    # calculate most popular set in train_set\n",
    "    recipeCount = defaultdict(int)\n",
    "    totalCooked = 0\n",
    "    for d in train_set:\n",
    "        recipeCount[d[1]] += 1\n",
    "        totalCooked += 1\n",
    "\n",
    "    mostPopular = [(recipeCount[x], x) for x in recipeCount]\n",
    "    mostPopular.sort()\n",
    "    mostPopular.reverse()\n",
    "\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > totalCooked*pt: break\n",
    "\n",
    "    # evalute on test_set\n",
    "    total_size=len(test_set)\n",
    "    correct_size=0\n",
    "    for i in range(total_size):\n",
    "        sample=test_set[i]\n",
    "        item=sample[1]\n",
    "        user=sample[0]\n",
    "        predict=0\n",
    "        \n",
    "         \n",
    "        maxSim=0\n",
    "        for d in itemsPerUser[user]:\n",
    "            sim=Jaccard(usersPerItem[d],usersPerItem[item])\n",
    "            maxSim=max(maxSim,sim)\n",
    "        if maxSim>jt or item in return1:\n",
    "            predict=1\n",
    "                \n",
    "        if predict==sample[2]:\n",
    "            correct_size+=1\n",
    "    accuracy=correct_size/total_size\n",
    "    print([accuracy,jt,pt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we incorporat the two threshhold, so we need to modified the similarity threshhold to 0.6, popularity threshhold to 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.675845, 0.6, 0.6]\n"
     ]
    }
   ],
   "source": [
    "jaccardPopularityModel(train_set,validate_set,0.6,0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00012578430211918068"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.675845/0.67576-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the model incorporating Jaccard and popularity on validation set's accuracy is 0.675845, which improved 0.126% from question 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. To run our model on the test set, we’ll have to use the files ‘stub Made.txt’ to find the user id/recipe id pairs about which we have to make predictions. Using that data, run the above model and upload your solution to Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload to kaggle, we can use the whole dataset as our train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=build_train_set(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify it to upload the result to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardPopularityModel(train_set, jt=0.01, pt=0.6):\n",
    "    print(\"predicting....\")\n",
    "    itemsPerUser = defaultdict(set)\n",
    "    usersPerItem=defaultdict(set)\n",
    "    for d in train_set:\n",
    "        user,item = d[0], d[1]\n",
    "        itemsPerUser[user].add(item)\n",
    "        usersPerItem[item].add(user)\n",
    "    \n",
    "    itemSet=set([d[1] for d in train_set])\n",
    "    userSet=set([d[0] for d in train_set])\n",
    "    \n",
    "    # calculate average number of recipes made in the train_set\n",
    "    averageNum=len(train_set)/len(userSet)\n",
    "\n",
    "    # calculate most popular set in train_set\n",
    "    recipeCount = defaultdict(int)\n",
    "    totalCooked = 0\n",
    "    for d in train_set:\n",
    "        recipeCount[d[1]] += 1\n",
    "        totalCooked += 1\n",
    "\n",
    "    mostPopular = [(recipeCount[x], x) for x in recipeCount]\n",
    "    mostPopular.sort()\n",
    "    mostPopular.reverse()\n",
    "\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > totalCooked*pt: break\n",
    "\n",
    "    predictions = open(\"predictions_Made.txt\", 'w')\n",
    "    for l in open(\"stub_Made.txt\"):\n",
    "        if l.startswith(\"user_id\"):\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        user,item = l.strip().split('-')\n",
    "        predict=0\n",
    "        \n",
    "        maxSim=0\n",
    "        for d in itemsPerUser[user]:\n",
    "            sim=Jaccard(usersPerItem[d],usersPerItem[item])\n",
    "            maxSim=max(maxSim,sim)\n",
    "        if maxSim>jt or item in return1:\n",
    "            predict=1\n",
    "                \n",
    "        if predict==1:\n",
    "            predictions.write(user + '-' + item + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(user + '-' + item + \",0\\n\")\n",
    "    predictions.close()\n",
    "    print(\"predicting finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=build_train_set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting....\n",
      "predicting finished!\n"
     ]
    }
   ],
   "source": [
    "jaccardPopularityModel(train_set,0.6,0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name on kaggle is Ethannewbee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks (Rating prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Fit a predict of the form\n",
    "$$\n",
    "\\text { rating (user, item ) } \\simeq \\alpha+\\beta_{\\text {user }}+\\beta_{\\text {item }}\n",
    "$$\n",
    "by fitting the mean and the two bias terms as described in the lecture notes. Use a regularization\n",
    "parameter of $\\lambda$ = 1. Report the MSE on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"assignment1/trainInteractions.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    c = csv.reader(f)\n",
    "    header = next(c)\n",
    "    for l in c:\n",
    "        d = dict(zip(header,l))\n",
    "        yield d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(readCSV(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '88348277',\n",
       " 'recipe_id': '03969194',\n",
       " 'date': '2004-12-23',\n",
       " 'rating': '5'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=dataset[:400000]\n",
    "validate_set=dataset[400000:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(d['rating']) for d in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "# reviewsPerUser = defaultdict(set)\n",
    "# reviewsPerItem = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_set:\n",
    "    user,item = d['user_id'], d['recipe_id']\n",
    "    reviewsPerUser[user].append(d)\n",
    "    reviewsPerItem[item].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5808"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingMean = sum([int(d['rating']) for d in train_set]) / len(train_set)\n",
    "ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(train_set)\n",
    "nUsers = len(reviewsPerUser)\n",
    "nItems = len(reviewsPerItem)\n",
    "users = list(reviewsPerUser.keys())\n",
    "items = list(reviewsPerItem.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_(user, item):\n",
    "    if user in userBiases and item in itemBiases:\n",
    "        pred=alpha + userBiases[user] + itemBiases[item]\n",
    "    elif user in userBiases:\n",
    "        pred=alpha + userBiases[user]\n",
    "    elif item in itemBiases:\n",
    "        pred=alpha + itemBiases[item]\n",
    "    else:\n",
    "        pred=alpha\n",
    "    if pred<0:\n",
    "        return 0\n",
    "    if pred>5.0:\n",
    "        return 5.0\n",
    "#     if 5-pred<0.2:\n",
    "#         return round(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(d['user_id'], d['recipe_id']) for d in train_set]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(train_set)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for d in train_set:\n",
    "        u,i = d['user_id'], d['recipe_id']\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - int(d['rating'])\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set $\\lambda$ =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.8987313599958769\n",
      "MSE = 0.8856358581692618\n",
      "MSE = 0.8985952813610849\n",
      "MSE = 0.8985952329948594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.58067734e+00, -8.60369441e-05, -8.06941551e-06, ...,\n",
       "        -1.45226856e-06,  1.04576614e-06, -1.45209452e-06]),\n",
       " 0.8986631878143104,\n",
       " {'grad': array([ 5.03931794e-07, -2.16526137e-07, -9.07182253e-09, ...,\n",
       "         -1.25235739e-09, -1.45141735e-10, -1.25490187e-09]),\n",
       "  'task': b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 4,\n",
       "  'nit': 2,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, lamb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training on train set, we evaludate the result on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9094108423649899"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "validate_truth=[int(d['rating']) for d in validate_set]\n",
    "for d in validate_set:\n",
    "    user=d['user_id']\n",
    "    item=d['recipe_id']\n",
    "    predictions.append(prediction_(user, item))\n",
    "\n",
    "MSE(predictions, validate_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the MSE on the validation set is 0.9094108423649899"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Report the user and recipe IDs that have the largest and smallest values of $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['32445558', 0.003670108443486672]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[max(userBiases, key=userBiases.get),max(userBiases.values())]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user ID with the largest $\\beta$ is 32445558"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['98124873', 0.00020946436381658414]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[max(itemBiases, key=itemBiases.get),max(itemBiases.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The item ID with the largest $\\beta$ is 98124873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['70705426', -0.0012946275802599271]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[min(userBiases, key=userBiases.get),min(userBiases.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user ID with the smallest $\\beta$ is 70705426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['29147042', -0.0002853192176457722]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[min(itemBiases, key=itemBiases.get),min(itemBiases.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The item ID with the smallest $\\beta$ is 29147042"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Find a better value of $\\lambda$ using your validation set. Report the value you chose, its MSE, and upload your solution to Kaggle by running it on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_mse():\n",
    "    predictions = []\n",
    "    validate_truth=[int(d['rating']) for d in validate_set]\n",
    "    for d in validate_set:\n",
    "        user=d['user_id']\n",
    "        item=d['recipe_id']\n",
    "        predictions.append(prediction_(user, item))\n",
    "\n",
    "    print(MSE(predictions, validate_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.909671527164235\n",
      "MSE = 1.6942491620860933\n",
      "MSE = 0.8985688354549487\n",
      "MSE = 0.898432691370945\n",
      "MSE = 0.8978924902631409\n",
      "MSE = 0.8958016894804687\n",
      "MSE = 0.8885585447239309\n",
      "MSE = 0.8606412718262736\n",
      "MSE = 0.8426554564331113\n",
      "MSE = 0.8254599045185007\n",
      "MSE = 0.8069106284337644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.48131120e+00, -1.09144661e-01, -2.91699317e-02, ...,\n",
       "        -5.60199519e-03,  2.76760356e-03, -4.98369526e-03]),\n",
       " 0.8094759995950068,\n",
       " {'grad': array([-2.00041981e-04, -1.27467781e-04,  9.36242455e-06, ...,\n",
       "          2.27880848e-06, -8.66824184e-07,  6.94887278e-07]),\n",
       "  'task': b'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT',\n",
       "  'funcalls': 11,\n",
       "  'nit': 6,\n",
       "  'warnflag': 1})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, 10**(-5)), maxiter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8470583195829219\n"
     ]
    }
   ],
   "source": [
    "validation_mse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.073610660966968"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9094108423649899/0.8470583195829219-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mse is 0.8470583195829219, which is an 7% improvement from question 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rated.txt\", 'w')\n",
    "for l in open(\"stub_Rated.txt\"):\n",
    "    if l.startswith(\"user_id\"):\n",
    "        \n",
    "      #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i = l.strip().split('-')\n",
    "    predictions.write(u + '-' + i + ',' + str(prediction_(u, i)) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitc282c26bf4d14ab391e9a3806fefddf8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
